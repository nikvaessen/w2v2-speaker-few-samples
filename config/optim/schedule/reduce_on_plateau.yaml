# the scheduler object to use
scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau

  # whether the monitored value is minimized or maximized
  mode: min

  # factor by which to reduce the lr when it has plateaued
  factor: 0.1

  # number of epochs with no improvement after which learning rate will be reduced.
  # Be careful with setting this value when also using early stopping
  patience: 3

  # Threshold for measuring the new optimum, to only focus on significant changes
  threshold: 1e-2

  # Number of epochs to wait before resuming normal operation after lr has been reduced
  cooldown:  0

  # A lower bound on the learning rate
  min_lr: 0

# optional value to track which is fed into the step() call
# only relevant for learning rate schedulers such
# as `reduce on plateau`
monitor: val_eer

# whether to step every epoch or every step
interval: epoch

# amount of epochs/steps between consecutive step() calls
frequency: null

# name to log the learning rate as
name: null