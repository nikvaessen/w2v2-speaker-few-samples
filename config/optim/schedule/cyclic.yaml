# the scheduler object to use
scheduler:
  _target_: torch.optim.lr_scheduler.CyclicLR

  # the lowest lr in the cycle
  base_lr: 1e-8

  # the peak lr in the cycle
  max_lr: ${optim.algo.lr}

  # number of steps to go from base_lr to max_lr
  step_size_up: ${idivide:${trainer.max_steps}, 8} # 4 cycles

  # number of steps to go from max+lr to base_lr
  step_size_down: ${idivide:${trainer.max_steps}, 8} # 4 cycles

  # Adam doesn't have `momentum` parameter, can only be true with SGD
  cycle_momentum: False

  # shape of line (triangular=linearly increasing/decreasing)
  mode: triangular2

# optional value to track which is fed into the step() call
# only relevant for learning rate schedulers such
# as `reduce on plateau`
monitor: null

# whether to step every epoch or every step
interval: step

# amount of epochs/steps between consecutive step() calls
frequency: null

# name to log the learning rate as
name: null