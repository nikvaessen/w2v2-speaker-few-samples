# the scheduler object to use
scheduler:
  _target_: torch.optim.lr_scheduler.OneCycleLR

  # maximum learning rate to reach in the cycle
  max_lr: ${optim.algo.lr}

  # the amount of steps in the training
  total_steps: ${trainer.max_steps}

  # the initial learning rate is max_lr / div_factor
  div_factor: 25

# optional value to track which is fed into the step() call
# only relevant for learning rate schedulers such
# as `reduce on plateau`
monitor: null

# whether to step every epoch or every step
interval: step

# amount of epochs/steps between consecutive step() calls
frequency: null

# name to log the learning rate as
name: null